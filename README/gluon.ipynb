{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gluon - Neural network building blocks\n",
    "\n",
    "Gluon package is a high-level interface for MXNet designed to be easy to use while\n",
    "keeping most of the flexibility of low level API. Gluon supports both imperative\n",
    "and symbolic programming, making it easy to train complex models imperatively\n",
    "in Python and then deploy with symbolic graph in C++ and Scala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import mxnet.ndarray as F\n",
    "import mxnet.gluon as gluon\n",
    "from mxnet.gluon import nn\n",
    "from mxnet import autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks (and other machine learning models) can be defined and trained\n",
    "with `gluon.nn` and `gluon.rnn` package. A typical training script has the following\n",
    "steps:\n",
    "\n",
    "- Define network\n",
    "- Initialize parameters\n",
    "- Loop over inputs\n",
    "- Forward input through network to get output\n",
    "- Compute loss with output and label\n",
    "- Backprop gradient\n",
    "- Update parameters with gradient descent.\n",
    "\n",
    "\n",
    "## Define Network\n",
    "\n",
    "`gluon.Block` is the basic building block of models. You can define networks by\n",
    "composing and inheriting `Block`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(gluon.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Net, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # layers created in name_scope will inherit name space\n",
    "            # from parent layer.\n",
    "            self.conv1 = nn.Conv2D(6, kernel_size=5)\n",
    "            self.pool1 = nn.MaxPool2D(pool_size=(2,2))\n",
    "            self.conv2 = nn.Conv2D(16, kernel_size=5)\n",
    "            self.pool2 = nn.MaxPool2D(pool_size=(2,2))\n",
    "            self.fc1 = nn.Dense(120)\n",
    "            self.fc2 = nn.Dense(84)\n",
    "            self.fc3 = nn.Dense(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        # 0 means copy over size from corresponding dimension.\n",
    "        # -1 means infer size from the rest of dimensions.\n",
    "        x = x.reshape((0, -1))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Parameters\n",
    "\n",
    "A network must be created and initialized before it can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "# Initialize on CPU. Replace with `mx.gpu(0)`, or `[mx.gpu(0), mx.gpu(1)]`,\n",
    "# etc to use one or more GPUs.\n",
    "net.initialize(mx.init.Xavier(), ctx=mx.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that because we didn't specify input size to layers in Net's constructor,\n",
    "the shape of parameters cannot be determined at this point. Actual initialization\n",
    "is deferred to the first forward pass, i.e. if you access `net.fc1.weight.data()`\n",
    "now an exception will be raised.\n",
    "\n",
    "You can actually initialize the weights by running a forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mx.nd.random_normal(shape=(10, 1, 32, 32))  # dummy data\n",
    "output = net(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can specify input size when creating layers, i.e. `nn.Dense(84, in_units=120)`\n",
    "instead of `nn.Dense(84)`.\n",
    "\n",
    "## Loss Functions\n",
    "\n",
    "Loss functions take (output, label) pairs and compute a scalar loss for each sample\n",
    "in the mini-batch. The scalars measure how far each output is from the label.\n",
    "\n",
    "There are many predefined loss functions in `gluon.loss`. Here we use\n",
    "`softmax_cross_entropy_loss` for digit classification.\n",
    "\n",
    "To compute loss and backprop for one iteration, we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = mx.nd.arange(10)  # dummy label\n",
    "with autograd.record():\n",
    "    output = net(data)\n",
    "    L = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    loss = L(output, label)\n",
    "    loss.backward()\n",
    "print('loss:', loss)\n",
    "print('grad:', net.fc1.weight.grad())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the weights\n",
    "\n",
    "Now that gradient is computed, we just need to update the weights. This is usually\n",
    "done with formulas like `weight = weight - learning_rate * grad / batch_size`.\n",
    "Note we divide gradient by batch_size because gradient is aggregated over the\n",
    "entire batch. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "for p in net.collect_params().values():\n",
    "    p.data()[:] -= lr / data.shape[0] * p.grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But sometimes you want more fancy updating rules like momentum and Adam, and since\n",
    "this is a commonly used functionality, gluon provide a `Trainer` class for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.01})\n",
    "\n",
    "with autograd.record():\n",
    "    output = net(data)\n",
    "    L = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    loss = L(output, label)\n",
    "    loss.backward()\n",
    "\n",
    "# do the update. Trainer needs to know the batch size of data to normalize\n",
    "# the gradient by 1/batch_size.\n",
    "trainer.step(data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<!-- INSERT SOURCE DOWNLOAD BUTTONS -->\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "display_name": "",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "name": ""
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
